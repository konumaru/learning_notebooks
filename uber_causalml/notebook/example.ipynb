{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rui/.local/share/virtualenvs/learning_notebooks-XrQCIa6W/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/rui/.local/share/virtualenvs/learning_notebooks-XrQCIa6W/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n"
     ]
    }
   ],
   "source": [
    "from causalml.inference.meta import LRSRegressor\n",
    "from causalml.inference.meta import XGBTRegressor, MLPTRegressor\n",
    "from causalml.inference.meta import BaseXRegressor\n",
    "from causalml.inference.meta import BaseRRegressor\n",
    "from causalml.dataset import synthetic_data\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Treatment Effect Estimation with S, T, X, and R Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータの生成\n",
    "y, X, treatment, _, _, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Treatment Effect (Linear Regression): 0.57 (0.42, 0.72)\n"
     ]
    }
   ],
   "source": [
    "lr = LRSRegressor()\n",
    "te, lb, ub = lr.estimate_ate(X, treatment, y)\n",
    "print('\\nAverage Treatment Effect (Linear Regression): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:16:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:16:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\n",
      "Average Treatment Effect (XGBoost): 0.52 (0.42, 0.62)\n"
     ]
    }
   ],
   "source": [
    "xg = XGBTRegressor(random_state=42)\n",
    "te, lb, ub = xg.estimate_ate(X, treatment, y)\n",
    "print('\\nAverage Treatment Effect (XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Treatment Effect (Neural Network (MLP)): 0.68 (0.55, 0.81)\n"
     ]
    }
   ],
   "source": [
    "nn = MLPTRegressor(\n",
    "    hidden_layer_sizes=(10, 10),\n",
    "    learning_rate_init=.1,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "te, lb, ub = nn.estimate_ate(X, treatment, y)\n",
    "print('\\nAverage Treatment Effect (Neural Network (MLP)): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:16:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:16:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:16:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:16:18] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\n",
      "Average Treatment Effect (BaseXRegressor using XGBoost): 0.46 (0.36, 0.55)\n"
     ]
    }
   ],
   "source": [
    "xl = BaseXRegressor(learner=XGBRegressor(random_state=42))\n",
    "te, lb, ub = xl.estimate_ate(X, e, treatment, y)\n",
    "print('\\nAverage Treatment Effect (BaseXRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:16:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\n",
      "Average Treatment Effect (BaseRRegressor using XGBoost): 0.45 (0.44, 0.45)\n"
     ]
    }
   ],
   "source": [
    "rl = BaseRRegressor(learner=XGBRegressor(random_state=42))\n",
    "te, lb, ub =  rl.estimate_ate(X=X, p=e, treatment=treatment, y=y)\n",
    "print('\\nAverage Treatment Effect (BaseRRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretable Causal ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータの生成\n",
    "y, X, treatment, _, _, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Control group level control not found in treatment vector.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9825b048adb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mslearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseSRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'control'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mslearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_ate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mslearner_tau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_notebooks-XrQCIa6W/lib/python3.7/site-packages/causalml/inference/meta/slearner.py\u001b[0m in \u001b[0;36mestimate_ate\u001b[0;34m(self, X, treatment, y, return_ci, bootstrap_ci, n_bootstraps, bootstrap_size)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[0minterval\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mATE\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_cs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_groups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_notebooks-XrQCIa6W/lib/python3.7/site-packages/causalml/inference/meta/slearner.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, treatment, y, return_ci, n_bootstraps, bootstrap_size, return_components, verbose)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mUB\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_treatment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_notebooks-XrQCIa6W/lib/python3.7/site-packages/causalml/inference/meta/slearner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, treatment, y)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[1;32m     77\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_pd_to_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mcheck_treatment_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreatment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtreatment\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_groups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_notebooks-XrQCIa6W/lib/python3.7/site-packages/causalml/inference/meta/utils.py\u001b[0m in \u001b[0;36mcheck_treatment_vector\u001b[0;34m(treatment, control_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontrol_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mcontrol_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;34m'Control group level {} not found in treatment vector.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Control group level control not found in treatment vector."
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from causalml.inference.meta import BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor\n",
    "\n",
    "slearner = BaseSRegressor(LGBMRegressor(), control_name='control')\n",
    "slearner.estimate_ate(X, w_multi, y)\n",
    "slearner_tau = slearner.fit_predict(X, w_multi, y)\n",
    "\n",
    "model_tau_feature = RandomForestRegressor()  # specify model for model_tau_feature\n",
    "\n",
    "slearner.get_importance(X=X, tau=slearner_tau, model_tau_feature=model_tau_feature,\n",
    "                        normalize=True, method='auto', features=feature_names)\n",
    "\n",
    "# Using the feature_importances_ method in the base learner (LGBMRegressor() in this example)\n",
    "slearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='auto')\n",
    "\n",
    "# Using eli5's PermutationImportance\n",
    "slearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='permutation')\n",
    "\n",
    "# Using SHAP\n",
    "shap_slearner = slearner.get_shap_values(X=X, tau=slearner_tau)\n",
    "\n",
    "# Plot shap values without specifying shap_dict\n",
    "slearner.plot_shap_values(X=X, tau=slearner_tau)\n",
    "\n",
    "# Plot shap values WITH specifying shap_dict\n",
    "slearner.plot_shap_values(shap_dict=shap_slearner)\n",
    "\n",
    "# interaction_idx set to 'auto' (searches for feature with greatest approximate interaction)\n",
    "slearner.plot_shap_dependence(treatment_group='treatment_A',\n",
    "                              feature_idx=1,\n",
    "                              X=X,\n",
    "                              tau=slearner_tau,\n",
    "                              interaction_idx='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_multi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9b8bb4b2969d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mslearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseSRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'control'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mslearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_ate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_multi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mslearner_tau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_multi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w_multi' is not defined"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from causalml.inference.meta import BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor\n",
    "\n",
    "slearner = BaseSRegressor(LGBMRegressor(), control_name='control')\n",
    "slearner.estimate_ate(X, w_multi, y)\n",
    "slearner_tau = slearner.fit_predict(X, w_multi, y)\n",
    "\n",
    "model_tau_feature = RandomForestRegressor()  # specify model for model_tau_feature\n",
    "\n",
    "slearner.get_importance(X=X, tau=slearner_tau, model_tau_feature=model_tau_feature,\n",
    "                        normalize=True, method='auto', features=feature_names)\n",
    "\n",
    "# Using the feature_importances_ method in the base learner (LGBMRegressor() in this example)\n",
    "slearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='auto')\n",
    "\n",
    "# Using eli5's PermutationImportance\n",
    "slearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='permutation')\n",
    "\n",
    "# Using SHAP\n",
    "shap_slearner = slearner.get_shap_values(X=X, tau=slearner_tau)\n",
    "\n",
    "# Plot shap values without specifying shap_dict\n",
    "slearner.plot_shap_values(X=X, tau=slearner_tau)\n",
    "\n",
    "# Plot shap values WITH specifying shap_dict\n",
    "slearner.plot_shap_values(shap_dict=shap_slearner)\n",
    "\n",
    "# interaction_idx set to 'auto' (searches for feature with greatest approximate interaction)\n",
    "slearner.plot_shap_dependence(treatment_group='treatment_A',\n",
    "                              feature_idx=1,\n",
    "                              X=X,\n",
    "                              tau=slearner_tau,\n",
    "                              interaction_idx='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9e60452814a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                     n_reg=100, evaluationFunction='KL', control_name='control')\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m uplift_model.fit(df[features].values,\n\u001b[0m\u001b[1;32m      9\u001b[0m                  \u001b[0mtreatment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'treatment_group_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                  y=df['conversion'].values)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier\n",
    "from causalml.inference.tree import uplift_tree_string, uplift_tree_plot\n",
    "\n",
    "uplift_model = UpliftTreeClassifier(max_depth=5, min_samples_leaf=200, min_samples_treatment=50,\n",
    "                                    n_reg=100, evaluationFunction='KL', control_name='control')\n",
    "\n",
    "uplift_model.fit(df[features].values,\n",
    "                 treatment=df['treatment_group_key'].values,\n",
    "                 y=df['conversion'].values)\n",
    "\n",
    "graph = uplift_tree_plot(uplift_model.fitted_uplift_tree, features)\n",
    "Image(graph.create_png())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
